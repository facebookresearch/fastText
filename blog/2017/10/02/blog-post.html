<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Language identification · fastText</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Language identification · fastText"/><meta property="og:type" content="website"/><meta property="og:url" content="fasttext.cc/blog/2017/10/02/blog-post.html"/><meta property="og:description" content="## Fast and accurate language identification using fastText"/><meta property="og:image" content="fasttext.cc/img/ogimage.png"/><link rel="shortcut icon" href="/img/fasttext-icon-bg-web.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="fasttext.cc/blog/atom.xml" title="fastText Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="fasttext.cc/blog/feed.xml" title="fastText Blog RSS Feed"/><link rel="stylesheet" href="/css/main.css"/></head><body class="sideNavVisible"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/fasttext-icon-white-web.png"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li><a href="/docs/en/support.html" target="_self">Docs</a></li><li><a href="/docs/en/english-vectors.html" target="_self">Download</a></li><li><a href="/blog" target="_self">Blog</a></li><li><a href="https://github.com/facebookresearch/fastText/" target="_blank">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Recent Posts</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Recent Posts</h3><ul><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/blog/2017/10/02/blog-post.html">Language identification</a></li><li class="navListItem"><a class="navItem" href="/blog/2017/05/02/blog-post.html">fastText on mobile</a></li><li class="navListItem"><a class="navItem" href="/blog/2016/08/18/blog-post.html">Releasing fastText</a></li></ul></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script></nav></div><div class="container mainContainer documentContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1><a href="/blog/2017/10/02/blog-post.html">Language identification</a></h1><p class="post-meta">October 2, 2017</p><div class="authorBlock"><p class="post-authorName"><a href="https://research.fb.com/people/grave-edouard/" target="_blank">Edouard Grave</a></p><div class="authorPhoto"><a href="https://research.fb.com/people/grave-edouard/" target="_blank"><img src="https://graph.facebook.com/534178442/picture/?height=200&amp;width=200"/></a></div></div></header><div><span><h2><a class="anchor" aria-hidden="true" name="fast-and-accurate-language-identification-using-fasttext"></a><a href="#fast-and-accurate-language-identification-using-fasttext" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fast and accurate language identification using fastText</h2>
<p>We are excited to announce that we are publishing a fast and accurate tool for text-based language identification. It can recognize more than 170 languages, takes less than 1MB of memory and can classify thousands of documents per second. It is based on fastText library and is released <a href="https://fasttext.cc/docs/en/language-identification.html">here</a> as open source, free to use by everyone. We are releasing several versions of the model, each optimized for different memory usage, and compared them to the popular tool <a href="https://github.com/saffsd/langid.py">langid.py</a>.</p>
<!--truncate-->
<p><img src="../../../../img/blog/2017-10-02-blog-post-img1.png" alt="Evaluation of our models"></p>
<p>Our tool uses various features offered by the fastText library, such as subwords or model compression. In the remainder of this blogpost, we will explain how these work, and how to use them to build a fast and small language detector.</p>
<h2><a class="anchor" aria-hidden="true" name="training-your-own-language-detector"></a><a href="#training-your-own-language-detector" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training your own language detector</h2>
<p>Building a fast and small language detector with fastText can be done with a few command lines, as we will show below. First, we need a dataset to train our model. Here, we propose to use sentences from the Tatoeba website, which can be downloaded from <a href="https://tatoeba.org/eng/downloads">https://tatoeba.org/eng/downloads</a>. Note that for the sake of simplicity, we use a small quantity of data for this blogpost . If you want to train a state-of-the-art model comparable with our pre-trained model, you will need to use a larger quantity of data.</p>
<h3><a class="anchor" aria-hidden="true" name="training-data"></a><a href="#training-data" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training data</h3>
<p>First, let's download the training data:</p>
<pre><code class="hljs css bash">&gt;&gt; wget http://downloads.tatoeba.org/exports/sentences.tar.bz2
&gt;&gt; bunzip2 sentences.tar.bz2
&gt;&gt; tar xvf sentences.tar
</code></pre>
<p>Then, we need to put our training data into fastText format, which is easily done using:</p>
<pre><code class="hljs css bash">&gt;&gt; awk -F<span class="hljs-string">"\t"</span> <span class="hljs-string">'{print"__label__"$2" "$3}'</span> &lt; sentences.csv | shuf &gt; all.txt
</code></pre>
<p>We can then split our training data into training and validation sets:</p>
<pre><code class="hljs css bash">&gt;&gt; head -n 10000 all.txt &gt; valid.txt
&gt;&gt; tail -n +10001 all.txt &gt; train.txt
</code></pre>
<h3><a class="anchor" aria-hidden="true" name="first-model"></a><a href="#first-model" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>First model</h3>
<p>We can now train our first model</p>
<pre><code class="hljs css bash">&gt;&gt; ./fasttext supervised -input train.txt -output langdetect -dim 16
</code></pre>
<p>and test it on the held out data:</p>
<pre><code class="hljs css bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> langdetect.bin valid.txt
</code></pre>
<p>This model should have an accuracy around 96.5%. Let's see if we can do better, by changing the default parameters.</p>
<h3><a class="anchor" aria-hidden="true" name="using-subword-features"></a><a href="#using-subword-features" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using subword features</h3>
<p>The first way to improve our baseline model is to use subword features, which enhance the classifier by taking into account the structure of words. It uses a simple, yet effective way of incorporating such information: each word is represented by the set of all character ngrams of a given length appearing in that word. As an example, when using subwords of length 3, the word skiing is represented by</p>
<pre><code class="hljs">{ skiing, ski, kii, iin, ing }
</code></pre>
<p>A key advantage of these features is that out-of-vocabulary words, such as misspelled words, can still be represented at test time by their subwords representations. This make text classifiers much more robust, especially for problems with small training sets, or for morphologically rich languages. Users can enable these features by simply specifying the value of the minimum and maximum character ngram size with the command line options -minn and -maxn:</p>
<pre><code class="hljs css bash">&gt;&gt; ./fasttext supervised -input train.txt -output langdetect -dim 16 -minn 2 -maxn 4
</code></pre>
<p>In that case, fastText now uses all the character ngrams of length 2, 3 and 4. The accuracy of the classifier should improve, and be above 98.5%. We can also make the training and testing faster, by using the hierarchical softmax:</p>
<pre><code class="hljs css bash">&gt;&gt; ./fasttext supervised -input train.txt -output langdetect -dim 16 -minn 2 -maxn 4 -loss hs
</code></pre>
<h3><a class="anchor" aria-hidden="true" name="model-compression"></a><a href="#model-compression" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model compression</h3>
<p>Finally, we can make the size of the model file much smaller, by using model compression:</p>
<pre><code class="hljs css bash">&gt;&gt; ./fasttext quantize -input train.txt -output langdetect -qnorm -cutoff 50000 -retrain
</code></pre>
<p>After running this command line, you should get a new model, langdetect.ftz, with a file size smaller than 1MB (instead of 350MB for the original model).</p>
<p>How does model quantization work? It is quite simple, and relies on two operations: weight quantization and feature selection. We now briefly describe these two operations in detail.</p>
<p><strong>Weight quantization.</strong> The first operation is to compress the weights of the models using a technique called vector quantization. Quantization is the process of mapping values from a large set (e.g. floating point numbers) to a smaller set (e.g. bytes). Here, we use a variant which is well suited to compress vectors, instead of scalar values. The algorithm, called product quantization, works as follow. First, each vector is split into smaller vectors, for example of dimension 2. Then, we run the k-means algorithm on these sub-vectors, and represent each sub-vector by the closest centroid obtained with k-means. Therefore, each 2-dimension vector is now represented by 1 byte (to store the centroid), instead of 8 bytes (to store the 2 floats), therefore achieving a compression rate of 8. If we instead split the vectors into sub-vectors of dimension 4, we can achieve a compression rate of 16 (but often with a higher distortion rate). This tradeoff between compression and distortion can be controlled using the -dsub command line option, which set the dimension of the sub-vectors.</p>
<p><strong>Feature selection.</strong> The second operation we apply to compress models is to remove features which do not have a big influence on the decision of the classifier. For this, our goal is to find the model with a given number of feature (e.g. 50,000 in the previous example) which is the closest from the original model. The solution of this problem is to keep the features (either words, subwords, or ngrams), which have the vectors with the largest norms.</p>
<h3><a class="anchor" aria-hidden="true" name="references"></a><a href="#references" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29">Quantization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vector_quantization">Vector quantization</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_selection">Feature selection</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" name="iso-codes-of-languages-supported"></a><a href="#iso-codes-of-languages-supported" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ISO codes of languages supported</h3>
<pre><code class="hljs">af als <span class="hljs-keyword">am</span> <span class="hljs-keyword">an</span> <span class="hljs-keyword">ar</span> arz <span class="hljs-keyword">as</span> ast av az azb <span class="hljs-keyword">ba</span> bar bcl <span class="hljs-keyword">be</span> bg bh <span class="hljs-keyword">bn</span> <span class="hljs-keyword">bo</span> bpy <span class="hljs-keyword">br</span> bs bxr <span class="hljs-keyword">ca</span> cbk <span class="hljs-keyword">ce</span> ceb ckb <span class="hljs-keyword">co</span> <span class="hljs-keyword">cs</span> cv cy da de diq dsb dty dv <span class="hljs-keyword">el</span> eml <span class="hljs-keyword">en</span> eo es et eu fa fi fr frr fy ga gd gl gn gom <span class="hljs-keyword">gu</span> <span class="hljs-keyword">gv</span> he <span class="hljs-keyword">hi</span> hif hr hsb ht hu hy <span class="hljs-keyword">ia</span> id ie ilo io <span class="hljs-keyword">is</span> it ja jbo jv ka kk km kn ko krc ku kv kw ky <span class="hljs-keyword">la</span> <span class="hljs-keyword">lb</span> lez li lmo <span class="hljs-keyword">lo</span> lrc <span class="hljs-keyword">lt</span> <span class="hljs-keyword">lv</span> mai mg mhr <span class="hljs-built_in">min</span> <span class="hljs-keyword">mk</span> ml mn mr mrj ms mt mwl my myv mzn nah nap nds ne <span class="hljs-keyword">new</span> nl <span class="hljs-keyword">nn</span> <span class="hljs-keyword">no</span> oc <span class="hljs-built_in">or</span> os pa pam pfl pl pms pnb <span class="hljs-keyword">ps</span> <span class="hljs-keyword">pt</span> qu rm ro <span class="hljs-keyword">ru</span> rue <span class="hljs-keyword">sa</span> sah sc scn sco sd <span class="hljs-keyword">sh</span> si sk <span class="hljs-keyword">sl</span> <span class="hljs-keyword">so</span> sq sr su <span class="hljs-keyword">sv</span> <span class="hljs-keyword">sw</span> <span class="hljs-keyword">ta</span> <span class="hljs-keyword">te</span> tg <span class="hljs-keyword">th</span> tk <span class="hljs-keyword">tl</span> <span class="hljs-keyword">tr</span> tt tyv ug uk ur uz vec vep <span class="hljs-keyword">vi</span> vls vo <span class="hljs-keyword">wa</span> war wuu xal xmf yi yo yue zh
</code></pre>
</span></div></div></div><div class="blog-recent"><a class="button" href="/blog">Recent Posts</a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/fasttext-icon-white-web.png" alt="fastText"/></a><div><h5>Support</h5><a href="/docs/en/support.html">Getting Started</a><a href="/docs/en/supervised-tutorial.html">Tutorials</a><a href="/docs/en/faqs.html">FAQs</a><a href="/docs/en/api.html">API</a></div><div><h5>Community</h5><a href="https://www.facebook.com/groups/1174547215919768/" target="_blank">Facebook Group</a><a href="http://stackoverflow.com/questions/tagged/fasttext" target="_blank">Stack Overflow</a><a href="https://groups.google.com/forum/#!forum/fasttext-library" target="_blank">Google Group</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/facebookresearch/fastText" target="_blank">GitHub</a><a class="github-button" href="https://github.com/facebookresearch/fastText/" data-icon="octicon-star" data-count-href="/fastText/stargazers" data-count-api="/repos/fastText#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://code.facebook.com/projects/" target="_blank" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2018 Facebook Inc.</section></footer></div><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-44373548-30', 'auto');
              ga('send', 'pageview');
            </script></body></html>