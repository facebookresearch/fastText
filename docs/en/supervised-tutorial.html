<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Text classification · fastText</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Text classification is a core problem to many applications, like spam detection, sentiment analysis or smart replies. In this tutorial, we describe how to build a text classifier with the fastText tool."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Text classification · fastText"/><meta property="og:type" content="website"/><meta property="og:url" content="https://fasttext.cc/index.html"/><meta property="og:description" content="Text classification is a core problem to many applications, like spam detection, sentiment analysis or smart replies. In this tutorial, we describe how to build a text classifier with the fastText tool."/><meta property="og:image" content="https://fasttext.cc/img/ogimage.png"/><meta name="twitter:card" content="summary"/><link rel="shortcut icon" href="/img/fasttext-icon-bg-web.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://fasttext.cc/blog/atom.xml" title="fastText Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://fasttext.cc/blog/feed.xml" title="fastText Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-44373548-30', 'auto');
              ga('send', 'pageview');
            </script><script type="text/javascript" src="/tabber.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/fasttext-icon-white-web.png" alt="fastText"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/docs/en/support.html" target="_self">Docs</a></li><li class=""><a href="/docs/en/english-vectors.html" target="_self">Resources</a></li><li class=""><a href="/blog/" target="_self">Blog</a></li><li class=""><a href="https://github.com/facebookresearch/fastText/" target="_blank">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Tutorials</span></h2></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Introduction</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/en/support.html">Get started</a></li><li class="navListItem"><a class="navItem" href="/docs/en/cheatsheet.html">Cheatsheet</a></li><li class="navListItem"><a class="navItem" href="/docs/en/options.html">List of options</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/docs/en/supervised-tutorial.html">Text classification</a></li><li class="navListItem"><a class="navItem" href="/docs/en/unsupervised-tutorial.html">Word representations</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Help</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/en/autotune.html">Automatic hyperparameter optimization</a></li><li class="navListItem"><a class="navItem" href="/docs/en/python-module.html">Python module</a></li><li class="navListItem"><a class="navItem" href="/docs/en/faqs.html">FAQ</a></li><li class="navListItem"><a class="navItem" href="/docs/en/api.html">API</a></li><li class="navListItem"><a class="navItem" href="/docs/en/references.html">References</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Text classification</h1></header><article><div><span><p>Text classification is a core problem to many applications, like spam detection, sentiment analysis or smart replies. In this tutorial, we describe how to build a text classifier with the fastText tool.</p>
<h2><a class="anchor" aria-hidden="true" id="what-is-text-classification"></a><a href="#what-is-text-classification" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is text classification?</h2>
<p>The goal of text classification is to assign documents (such as emails,  posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is  machine learning, that is  learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).</p>
<p>As an example, we build a classifier which automatically classifies stackexchange questions about cooking into one of  several possible tags, such as <code>pot</code>, <code>bowl</code> or <code>baking</code>.</p>
<h2><a class="anchor" aria-hidden="true" id="installing-fasttext"></a><a href="#installing-fasttext" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installing fastText</h2>
<p>The first step of this tutorial is to install and build fastText. It only requires a c++ compiler with good support of c++11.</p>
<p>Let us start by downloading the <a href="https://github.com/facebookresearch/fastText/releases">most recent release</a>:</p>
<pre><code class="hljs css language-bash">$ wget https://github.com/facebookresearch/fastText/archive/v0.9.1.zip
$ unzip v0.9.1.zip
</code></pre>
<p>Move to the fastText directory and build it:</p>
<pre><code class="hljs css language-bash">$ <span class="hljs-built_in">cd</span> fastText-0.9.1
<span class="hljs-comment"># for command line tool :</span>
$ make
<span class="hljs-comment"># for python bindings :</span>
$ pip install .
</code></pre>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-16-tab-17" class="nav-link active" data-group="group_16" data-tab="tab-group-16-content-17">Command line</div><div id="tab-group-16-tab-18" class="nav-link" data-group="group_16" data-tab="tab-group-16-content-18">Python</div></div><div class="tab-content"><div id="tab-group-16-content-17" class="tab-pane active" data-group="group_16" tabindex="-1"><div><span><p><br />
Running the binary without any argument will print the high level documentation, showing the different use cases supported by fastText:</p>
<pre><code class="hljs css language-bash">&gt;&gt; ./fasttext<br />usage: fasttext &lt;<span class="hljs-built_in">command</span>&gt; &lt;args&gt;<br /><br />The commands supported by fasttext are:<br /><br />  supervised              train a supervised classifier<br />  quantize                quantize a model to reduce the memory usage<br />  <span class="hljs-built_in">test</span>                    evaluate a supervised classifier<br />  predict                 predict most likely labels<br />  predict-prob            predict most likely labels with probabilities<br />  skipgram                train a skipgram model<br />  cbow                    train a cbow model<br />  <span class="hljs-built_in">print</span>-word-vectors      <span class="hljs-built_in">print</span> word vectors given a trained model<br />  <span class="hljs-built_in">print</span>-sentence-vectors  <span class="hljs-built_in">print</span> sentence vectors given a trained model<br />  nn                      query <span class="hljs-keyword">for</span> nearest neighbors<br />  analogies               query <span class="hljs-keyword">for</span> analogies<br /><br /></code></pre>
<p>In this tutorial, we mainly use the <code>supervised</code>, <code>test</code> and <code>predict</code> subcommands, which corresponds to learning (and using) text classifier. For an introduction to the other functionalities of fastText, please see the <a href="https://fasttext.cc/docs/en/unsupervised-tutorial.html">tutorial about learning word vectors</a>.</p>
</span></div></div><div id="tab-group-16-content-18" class="tab-pane" data-group="group_16" tabindex="-1"><div><span><p><br />
Calling the help function will show high level documentation of the library:</p>
<pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> fasttext<br /><span class="hljs-meta">&gt;&gt;&gt; </span>help(fasttext.FastText)<br />Help on module fasttext.FastText <span class="hljs-keyword">in</span> fasttext:<br /><br />NAME<br />    fasttext.FastText<br /><br />DESCRIPTION<br />    <span class="hljs-comment"># Copyright (c) 2017-present, Facebook, Inc.</span><br />    <span class="hljs-comment"># All rights reserved.</span><br />    <span class="hljs-comment">#</span><br />    <span class="hljs-comment"># This source code is licensed under the MIT license found in the</span><br />    <span class="hljs-comment"># LICENSE file in the root directory of this source tree.</span><br /><br />FUNCTIONS<br />    load_model(path)<br />        Load a model given a filepath <span class="hljs-keyword">and</span> <span class="hljs-keyword">return</span> a model object.<br />    <br />    read_args(arg_list, arg_dict, arg_names, default_values)<br />    <br />    tokenize(text)<br />        Given a string of text, tokenize it <span class="hljs-keyword">and</span> <span class="hljs-keyword">return</span> a list of tokens<br />    <br />    train_supervised(*kargs, **kwargs)<br />        Train a supervised model <span class="hljs-keyword">and</span> <span class="hljs-keyword">return</span> a model object.<br />        <br />        input must be a filepath. The input text does <span class="hljs-keyword">not</span> need to be tokenized<br />        <span class="hljs-keyword">as</span> per the tokenize function, but it must be preprocessed <span class="hljs-keyword">and</span> encoded<br />        <span class="hljs-keyword">as</span> UTF<span class="hljs-number">-8.</span> You might want to consult standard preprocessing scripts such<br />        <span class="hljs-keyword">as</span> tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html<br />        <br />        The input file must must contain at least one label per line. For an<br />        example consult the example datasets which are part of the fastText<br />        repository such <span class="hljs-keyword">as</span> the dataset pulled by classification-example.sh.<br />    <br />    train_unsupervised(*kargs, **kwargs)<br />        Train an unsupervised model <span class="hljs-keyword">and</span> <span class="hljs-keyword">return</span> a model object.<br />        <br />        input must be a filepath. The input text does <span class="hljs-keyword">not</span> need to be tokenized<br />        <span class="hljs-keyword">as</span> per the tokenize function, but it must be preprocessed <span class="hljs-keyword">and</span> encoded<br />        <span class="hljs-keyword">as</span> UTF<span class="hljs-number">-8.</span> You might want to consult standard preprocessing scripts such<br />        <span class="hljs-keyword">as</span> tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html<br />        <br />        The input field must <span class="hljs-keyword">not</span> contain any labels <span class="hljs-keyword">or</span> use the specified label prefix<br />        unless it <span class="hljs-keyword">is</span> ok <span class="hljs-keyword">for</span> those words to be ignored. For an example consult the<br />        dataset pulled by the example script word-vector-example.sh, which <span class="hljs-keyword">is</span><br />        part of the fastText repository.<br /></code></pre>
<p>In this tutorial, we mainly use the <code>train_supervised</code>, which returns a model object, and call <code>test</code> and <code>predict</code> on this object. That corresponds to learning (and using) text classifier. For an introduction to the other functionalities of fastText, please see the <a href="https://fasttext.cc/docs/en/unsupervised-tutorial.html">tutorial about learning word vectors</a>.</p>
</span></div></div></div></div>
<h2><a class="anchor" aria-hidden="true" id="getting-and-preparing-the-data"></a><a href="#getting-and-preparing-the-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting and preparing the data</h2>
<p>As mentioned in the introduction, we need labeled data to train our supervised classifier. In this tutorial, we are interested in building a classifier to automatically recognize the topic of a stackexchange question about cooking. Let's download examples of questions from <a href="http://cooking.stackexchange.com/">the cooking section of Stackexchange</a>, and their associated tags:</p>
<pre><code class="hljs css language-bash">&gt;&gt; wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz &amp;&amp; tar xvzf cooking.stackexchange.tar.gz
&gt;&gt; head cooking.stackexchange.txt
</code></pre>
<p>Each line of the text file contains a list of labels, followed by the corresponding document. All the labels start by the <code>__label__</code> prefix, which is how fastText recognize what is a label or what is a word. The model is then trained to predict the labels given the word in the document.</p>
<p>Before training our first classifier, we need to split the data into train and validation. We will use the validation set to evaluate how good the learned classifier is on new data.</p>
<pre><code class="hljs css language-bash">&gt;&gt; wc cooking.stackexchange.txt
   15404  169582 1401900 cooking.stackexchange.txt
</code></pre>
<p>Our full dataset contains 15404 examples. Let's split it into a training set of 12404 examples and a validation set of 3000 examples:</p>
<pre><code class="hljs css language-bash">&gt;&gt; head -n 12404 cooking.stackexchange.txt &gt; cooking.train
&gt;&gt; tail -n 3000 cooking.stackexchange.txt &gt; cooking.valid
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="our-first-classifier"></a><a href="#our-first-classifier" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Our first classifier</h2>
<p>We are now ready to train our first classifier:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-19-tab-20" class="nav-link active" data-group="group_19" data-tab="tab-group-19-content-20">Command line</div><div id="tab-group-19-tab-21" class="nav-link" data-group="group_19" data-tab="tab-group-19-content-21">Python</div></div><div class="tab-content"><div id="tab-group-19-content-20" class="tab-pane active" data-group="group_19" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking<br />Read 0M words<br />Number of words:  14598<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 75109  lr: 0.000000  loss: 5.708354  eta: 0h0m<br /></code></pre>
<p>The <code>-input</code> command line option indicates the file containing the training examples, while the <code>-output</code> option indicates where to save the model. At the end of training, a file <code>model_cooking.bin</code>, containing the trained classifier, is created in the current directory.</p>
</span></div></div><div id="tab-group-19-content-21" class="tab-pane" data-group="group_19" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> fasttext<br /><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">14598</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">75109</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">5.708354</span>  eta: <span class="hljs-number">0</span>h0m<br /></code></pre>
<p>The <code>input</code> argument indicates the file containing the training examples. We can now use the <code>model</code> variable to access information on the trained model.</p>
<p>We can also call <code>save_model</code> to save it as a file and load it later with <code>load_model</code> function.</p>
<pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.save_model(<span class="hljs-string">"model_cooking.bin"</span>)<br /></code></pre>
</span></div></div></div></div>
<p>Now, we can test our classifier, by :</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-22-tab-23" class="nav-link active" data-group="group_22" data-tab="tab-group-22-content-23">Command line</div><div id="tab-group-22-tab-24" class="nav-link" data-group="group_22" data-tab="tab-group-22-content-24">Python</div></div><div class="tab-content"><div id="tab-group-22-content-23" class="tab-pane active" data-group="group_22" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext predict model_cooking.bin -<br /></code></pre>
<p>and then typing a sentence.  Let's first try the sentence:</p>
<p><em>Which baking dish is best to bake a banana bread ?</em></p>
<p>The predicted tag is <code>baking</code>  which fits well to this question. Let us now try a second example:</p>
<p><em>Why not put knives in the dishwasher?</em></p>
</span></div></div><div id="tab-group-22-content-24" class="tab-pane" data-group="group_22" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.predict(<span class="hljs-string">"Which baking dish is best to bake a banana bread ?"</span>)<br />((<span class="hljs-string">u'__label__baking'</span>,), array([<span class="hljs-number">0.15613931</span>]))<br /></code></pre>
<p>The predicted tag is <code>baking</code>  which fits well to this question. Let us now try a second example:</p>
<pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.predict(<span class="hljs-string">"Why not put knives in the dishwasher?"</span>)<br />((<span class="hljs-string">u'__label__food-safety'</span>,), array([<span class="hljs-number">0.08686075</span>]))<br /></code></pre>
</span></div></div></div></div>
<p>The label predicted by the model is <code>food-safety</code>, which is not relevant. Somehow, the model seems to fail on simple examples.</p>
<p>To get a better sense of its quality, let's test it on the validation data by running:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-25-tab-26" class="nav-link active" data-group="group_25" data-tab="tab-group-25-content-26">Command line</div><div id="tab-group-25-tab-27" class="nav-link" data-group="group_25" data-tab="tab-group-25-content-27">Python</div></div><div class="tab-content"><div id="tab-group-25-content-26" class="tab-pane active" data-group="group_25" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.124<br />R@1  0.0541<br />Number of examples: 3000<br /></code></pre>
<p>The output of fastText are the precision at one (<code>P@1</code>) and the recall at one (<code>R@1</code>).</p>
</span></div></div><div id="tab-group-25-content-27" class="tab-pane" data-group="group_25" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.124</span>, <span class="hljs-number">0.0541</span>)<br /></code></pre>
<p>The output are the number of samples (here <code>3000</code>), the precision at one (<code>0.124</code>) and the recall at one (<code>0.0541</code>).</p>
</span></div></div></div></div>
<p>We can also compute the precision at five and recall at five with:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-28-tab-29" class="nav-link active" data-group="group_28" data-tab="tab-group-28-content-29">Command line</div><div id="tab-group-28-tab-30" class="nav-link" data-group="group_28" data-tab="tab-group-28-content-30">Python</div></div><div class="tab-content"><div id="tab-group-28-content-29" class="tab-pane active" data-group="group_28" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid 5<br />N  3000<br />P@5  0.0668<br />R@5  0.146<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-28-content-30" class="tab-pane" data-group="group_28" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>, k=<span class="hljs-number">5</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.0668</span>, <span class="hljs-number">0.146</span>)<br /></code></pre>
</span></div></div></div></div>
<h2><a class="anchor" aria-hidden="true" id="advanced-readers-precision-and-recall"></a><a href="#advanced-readers-precision-and-recall" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Advanced readers: precision and recall</h2>
<p>The precision is the number of correct labels among the labels predicted by fastText. The recall is the number of labels that successfully were predicted, among all the real labels. Let's take an example to make this more clear:</p>
<p><em>Why not put knives in the dishwasher?</em></p>
<p>On Stack Exchange, this sentence is labeled with three tags: <code>equipment</code>, <code>cleaning</code> and <code>knives</code>. The top five labels predicted by the model can be obtained with:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-31-tab-32" class="nav-link active" data-group="group_31" data-tab="tab-group-31-content-32">Command line</div><div id="tab-group-31-tab-33" class="nav-link" data-group="group_31" data-tab="tab-group-31-content-33">Python</div></div><div class="tab-content"><div id="tab-group-31-content-32" class="tab-pane active" data-group="group_31" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext predict model_cooking.bin - 5<br /></code></pre>
</span></div></div><div id="tab-group-31-content-33" class="tab-pane" data-group="group_31" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.predict(<span class="hljs-string">"Why not put knives in the dishwasher?"</span>, k=<span class="hljs-number">5</span>)<br />((<span class="hljs-string">u'__label__food-safety'</span>, <span class="hljs-string">u'__label__baking'</span>, <span class="hljs-string">u'__label__equipment'</span>, <span class="hljs-string">u'__label__substitutions'</span>, <span class="hljs-string">u'__label__bread'</span>), array([<span class="hljs-number">0.0857</span> , <span class="hljs-number">0.0657</span>, <span class="hljs-number">0.0454</span>, <span class="hljs-number">0.0333</span>, <span class="hljs-number">0.0333</span>]))<br /></code></pre>
</span></div></div></div></div>
<p>are <code>food-safety</code>, <code>baking</code>, <code>equipment</code>, <code>substitutions</code> and <code>bread</code>.</p>
<p>Thus, one out of five labels predicted by the model is correct, giving a precision of 0.20. Out of the three real labels, only one is predicted by the model, giving a recall of 0.33.</p>
<p>For more details, see <a href="https://en.wikipedia.org/wiki/Precision_and_recall">the related Wikipedia page</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="making-the-model-better"></a><a href="#making-the-model-better" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Making the model better</h2>
<p>The model obtained by running fastText with the default arguments is pretty bad at classifying new questions. Let's try to improve the performance, by changing the default parameters.</p>
<h3><a class="anchor" aria-hidden="true" id="preprocessing-the-data"></a><a href="#preprocessing-the-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>preprocessing the data</h3>
<p>Looking at the data, we observe that some words contain uppercase letter or punctuation. One of the first step to improve the performance of our model is to apply some simple pre-processing. A crude normalization can be obtained using command line tools such as <code>sed</code> and <code>tr</code>:</p>
<pre><code class="hljs css language-bash">&gt;&gt; cat cooking.stackexchange.txt | sed -e <span class="hljs-string">"s/\([.\!?,'/()]\)/ \1 /g"</span> | tr <span class="hljs-string">"[:upper:]"</span> <span class="hljs-string">"[:lower:]"</span> &gt; cooking.preprocessed.txt
&gt;&gt; head -n 12404 cooking.preprocessed.txt &gt; cooking.train
&gt;&gt; tail -n 3000 cooking.preprocessed.txt &gt; cooking.valid
</code></pre>
<p>Let's train a new model on the pre-processed data:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-34-tab-35" class="nav-link active" data-group="group_34" data-tab="tab-group-34-content-35">Command line</div><div id="tab-group-34-tab-36" class="nav-link" data-group="group_34" data-tab="tab-group-34-content-36">Python</div></div><div class="tab-content"><div id="tab-group-34-content-35" class="tab-pane active" data-group="group_34" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking<br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 82041  lr: 0.000000  loss: 5.671649  eta: 0h0m<br /><br />&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.164<br />R@1  0.0717<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-34-content-36" class="tab-pane" data-group="group_34" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> fasttext<br /><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">82041</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">5.671649</span>  eta: <span class="hljs-number">0</span>h0m<br /><br /><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.164</span>, <span class="hljs-number">0.0717</span>)<br /></code></pre>
</span></div></div></div></div>
<p>We observe that thanks to the pre-processing, the vocabulary is smaller (from 14k words to 9k). The precision is also starting to go up by 4%!</p>
<h3><a class="anchor" aria-hidden="true" id="more-epochs-and-larger-learning-rate"></a><a href="#more-epochs-and-larger-learning-rate" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>more epochs and larger learning rate</h3>
<p>By default, fastText sees each training example only five times during training, which is pretty small, given that our training set only have 12k training examples. The number of times each examples is seen (also known as the number of epochs), can be increased using the <code>-epoch</code> option:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-37-tab-38" class="nav-link active" data-group="group_37" data-tab="tab-group-37-content-38">Command line</div><div id="tab-group-37-tab-39" class="nav-link" data-group="group_37" data-tab="tab-group-37-content-39">Python</div></div><div class="tab-content"><div id="tab-group-37-content-38" class="tab-pane active" data-group="group_37" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -epoch 25<br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 77633  lr: 0.000000  loss: 7.147976  eta: 0h0m<br /></code></pre>
</span></div></div><div id="tab-group-37-content-39" class="tab-pane" data-group="group_37" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> fasttext<br /><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, epoch=<span class="hljs-number">25</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">77633</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">7.147976</span>  eta: <span class="hljs-number">0</span>h0m<br /></code></pre>
</span></div></div></div></div>
<p>Let's test the new model:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-40-tab-41" class="nav-link active" data-group="group_40" data-tab="tab-group-40-content-41">Command line</div><div id="tab-group-40-tab-42" class="nav-link" data-group="group_40" data-tab="tab-group-40-content-42">Python</div></div><div class="tab-content"><div id="tab-group-40-content-41" class="tab-pane active" data-group="group_40" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.501<br />R@1  0.218<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-40-content-42" class="tab-pane" data-group="group_40" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.501</span>, <span class="hljs-number">0.218</span>)<br /></code></pre>
</span></div></div></div></div>
<p>This is much better! Another way to change the learning speed of our model is to increase (or decrease) the learning rate of the algorithm. This corresponds to how much the model changes after processing each example. A learning rate of 0 would mean that the model does not change at all, and thus, does not learn anything. Good values of the learning rate are in the range <code>0.1 - 1.0</code>.</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-43-tab-44" class="nav-link active" data-group="group_43" data-tab="tab-group-43-content-44">Command line</div><div id="tab-group-43-tab-45" class="nav-link" data-group="group_43" data-tab="tab-group-43-content-45">Python</div></div><div class="tab-content"><div id="tab-group-43-content-44" class="tab-pane active" data-group="group_43" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -lr 1.0  <br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 81469  lr: 0.000000  loss: 6.405640  eta: 0h0m<br /><br />&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.563<br />R@1  0.245<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-43-content-45" class="tab-pane" data-group="group_43" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, lr=<span class="hljs-number">1.0</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">81469</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">6.405640</span>  eta: <span class="hljs-number">0</span>h0m<br /><br /><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.563</span>, <span class="hljs-number">0.245</span>)<br /></code></pre>
</span></div></div></div></div>
<p>Even better! Let's try both together:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-46-tab-47" class="nav-link active" data-group="group_46" data-tab="tab-group-46-content-47">Command line</div><div id="tab-group-46-tab-48" class="nav-link" data-group="group_46" data-tab="tab-group-46-content-48">Python</div></div><div class="tab-content"><div id="tab-group-46-content-47" class="tab-pane active" data-group="group_46" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25<br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 76394  lr: 0.000000  loss: 4.350277  eta: 0h0m<br /><br />&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.585<br />R@1  0.255<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-46-content-48" class="tab-pane" data-group="group_46" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, lr=<span class="hljs-number">1.0</span>, epoch=<span class="hljs-number">25</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">76394</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">4.350277</span>  eta: <span class="hljs-number">0</span>h0m<br /><br /><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.585</span>, <span class="hljs-number">0.255</span>)<br /></code></pre>
</span></div></div></div></div>
<p>Let us now add a few more features to improve even further our performance!</p>
<h3><a class="anchor" aria-hidden="true" id="word-n-grams"></a><a href="#word-n-grams" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>word n-grams</h3>
<p>Finally, we can improve the performance of a model by using word bigrams, instead of just unigrams. This is especially important for classification problems where word order is important, such as sentiment analysis.</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-49-tab-50" class="nav-link active" data-group="group_49" data-tab="tab-group-49-content-50">Command line</div><div id="tab-group-49-tab-51" class="nav-link" data-group="group_49" data-tab="tab-group-49-content-51">Python</div></div><div class="tab-content"><div id="tab-group-49-content-50" class="tab-pane active" data-group="group_49" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25 -wordNgrams 2<br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 75366  lr: 0.000000  loss: 3.226064  eta: 0h0m<br /><br />&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid<br />N  3000<br />P@1  0.599<br />R@1  0.261<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-49-content-51" class="tab-pane" data-group="group_49" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, lr=<span class="hljs-number">1.0</span>, epoch=<span class="hljs-number">25</span>, wordNgrams=<span class="hljs-number">2</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">75366</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">3.226064</span>  eta: <span class="hljs-number">0</span>h0m<br /><br /><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.599</span>, <span class="hljs-number">0.261</span>)<br /></code></pre>
</span></div></div></div></div>
<p>With a few steps, we were able to go from a precision at one of 12.4% to 59.9%. Important steps included:</p>
<ul>
<li>preprocessing the data ;</li>
<li>changing the number of epochs (using the option <code>-epoch</code>, standard range <code>[5 - 50]</code>) ;</li>
<li>changing the learning rate (using the option <code>-lr</code>, standard range <code>[0.1 - 1.0]</code>) ;</li>
<li>using word n-grams (using the option <code>-wordNgrams</code>, standard range <code>[1 - 5]</code>).</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="advanced-readers-what-is-a-bigram"></a><a href="#advanced-readers-what-is-a-bigram" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Advanced readers: What is a Bigram?</h2>
<p>A 'unigram' refers to a single undividing unit, or token,  usually used as an input to a model. For example a unigram can be a word or a letter depending on the model. In fastText, we work at the word level and thus unigrams are words.</p>
<p>Similarly we denote by 'bigram' the concatenation of  2 consecutive tokens or words. Similarly we often talk about n-gram to refer to the concatenation any n consecutive tokens.</p>
<p>For example, in the sentence, 'Last donut of the night', the unigrams are  'last', 'donut', 'of', 'the' and 'night'. The bigrams are: 'Last donut', 'donut of', 'of the' and 'the night'.</p>
<p>Bigrams are particularly interesting because, for most sentences, you can reconstruct the order of the words just by looking at a bag of n-grams.</p>
<p>Let us illustrate this by a simple exercise, given the following bigrams, try to reconstruct the original sentence: 'all out',  'I am', 'of bubblegum', 'out of' and 'am all'.
It is common to refer to a word as a unigram.</p>
<h2><a class="anchor" aria-hidden="true" id="scaling-things-up"></a><a href="#scaling-things-up" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scaling things up</h2>
<p>Since we are training our model on a few thousands of examples, the training only takes a few seconds. But training models on larger datasets, with more labels can start to be too slow. A potential solution to make the training faster is to use the <a href="#advanced-readers-hierarchical-softmax">hierarchical softmax</a>, instead of the regular softmax. This can be done with the option <code>-loss hs</code>:</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-52-tab-53" class="nav-link active" data-group="group_52" data-tab="tab-group-52-content-53">Command line</div><div id="tab-group-52-tab-54" class="nav-link" data-group="group_52" data-tab="tab-group-52-content-54">Python</div></div><div class="tab-content"><div id="tab-group-52-content-53" class="tab-pane active" data-group="group_52" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 -loss hs<br />Read 0M words<br />Number of words:  9012<br />Number of labels: 734<br />Progress: 100.0%  words/sec/thread: 2199406  lr: 0.000000  loss: 1.718807  eta: 0h0m<br /></code></pre>
</span></div></div><div id="tab-group-52-content-54" class="tab-pane" data-group="group_52" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, lr=<span class="hljs-number">1.0</span>, epoch=<span class="hljs-number">25</span>, wordNgrams=<span class="hljs-number">2</span>, bucket=<span class="hljs-number">200000</span>, dim=<span class="hljs-number">50</span>, loss=<span class="hljs-string">'hs'</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">9012</span><br />Number of labels: <span class="hljs-number">734</span><br />Progress: <span class="hljs-number">100.0</span>%  words/sec/thread: <span class="hljs-number">2199406</span>  lr: <span class="hljs-number">0.000000</span>  loss: <span class="hljs-number">1.718807</span>  eta: <span class="hljs-number">0</span>h0m<br /></code></pre>
</span></div></div></div></div>
<p>Training should now take less than a second.</p>
<h2><a class="anchor" aria-hidden="true" id="advanced-readers-hierarchical-softmax"></a><a href="#advanced-readers-hierarchical-softmax" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Advanced readers: hierarchical softmax</h2>
<p>The hierarchical softmax is a loss function that approximates the softmax with a much faster computation.</p>
<p>The idea is to build a binary tree whose leaves correspond to the labels. Each intermediate node has a binary decision activation (e.g. sigmoid) that is trained, and predicts if we should go to the left or to the right. The probability of the output unit is then given by the product of the probabilities of intermediate nodes along the path from the root to the output unit leave.</p>
<p>For a detailed explanation, you can have a look on <a href="https://www.youtube.com/watch?v=B95LTf2rVWM">this video</a>.</p>
<p>In fastText, we use a Huffman tree, so that the lookup time is faster for more frequent outputs and thus the average lookup time for the output is optimal.</p>
<h2><a class="anchor" aria-hidden="true" id="multi-label-classification"></a><a href="#multi-label-classification" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-label classification</h2>
<p>When we want to assign a document to multiple labels, we can still use the softmax loss and play with the parameters for prediction, namely the number of labels to predict and the threshold for the predicted probability. However playing with these arguments can be tricky and unintuitive since the probabilities must sum to 1.</p>
<p>A convenient way to handle multiple labels is to use independent binary classifiers for each label. This can be done with <code>-loss one-vs-all</code> or <code>-loss ova</code>.</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-55-tab-56" class="nav-link active" data-group="group_55" data-tab="tab-group-55-content-56">Command line</div><div id="tab-group-55-tab-57" class="nav-link" data-group="group_55" data-tab="tab-group-55-content-57">Python</div></div><div class="tab-content"><div id="tab-group-55-content-56" class="tab-pane active" data-group="group_55" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext supervised -input cooking.train -output model_cooking -lr 0.5 -epoch 25 -wordNgrams 2 -bucket 200000 -dim 50 -loss one-vs-all<br />Read 0M words<br />Number of words:  14543<br />Number of labels: 735<br />Progress: 100.0% words/sec/thread:   72104 lr:  0.000000 loss:  4.340807 ETA:   0h 0m<br /></code></pre>
</span></div></div><div id="tab-group-55-content-57" class="tab-pane" data-group="group_55" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> fasttext<br /><span class="hljs-meta">&gt;&gt;&gt; </span>model = fasttext.train_supervised(input=<span class="hljs-string">"cooking.train"</span>, lr=<span class="hljs-number">0.5</span>, epoch=<span class="hljs-number">25</span>, wordNgrams=<span class="hljs-number">2</span>, bucket=<span class="hljs-number">200000</span>, dim=<span class="hljs-number">50</span>, loss=<span class="hljs-string">'ova'</span>)<br />Read <span class="hljs-number">0</span>M words<br />Number of words:  <span class="hljs-number">14543</span><br />Number of labels: <span class="hljs-number">735</span><br />Progress: <span class="hljs-number">100.0</span>% words/sec/thread:   <span class="hljs-number">72104</span> lr:  <span class="hljs-number">0.000000</span> loss:  <span class="hljs-number">4.340807</span> ETA:   <span class="hljs-number">0</span>h <span class="hljs-number">0</span>m<br /></code></pre>
</span></div></div></div></div>
<p>It is a good idea to decrease the learning rate compared to other loss functions.</p>
<p>Now let's have a look on our predictions, we want as many prediction as possible (argument <code>-1</code>) and we want only labels with probability higher or equal to <code>0.5</code> :</p>
<div class="tabs"><div class="nav-tabs"><div id="tab-group-58-tab-59" class="nav-link active" data-group="group_58" data-tab="tab-group-58-content-59">Command line</div><div id="tab-group-58-tab-60" class="nav-link" data-group="group_58" data-tab="tab-group-58-content-60">Python</div></div><div class="tab-content"><div id="tab-group-58-content-59" class="tab-pane active" data-group="group_58" tabindex="-1"><div><span><pre><code class="hljs css language-bash">&gt;&gt; ./fasttext predict-prob model_cooking.bin - -1 0.5<br /></code></pre>
<p>and then type the sentence:</p>
<p><em>Which baking dish is best to bake a banana bread ?</em></p>
<p>we get:</p>
<pre><code class="hljs">__label__baking <span class="hljs-number">1.00000</span> __label__bananas <span class="hljs-number">0.939923</span> __label__bread <span class="hljs-number">0.592677</span><br /></code></pre>
</span></div></div><div id="tab-group-58-content-60" class="tab-pane" data-group="group_58" tabindex="-1"><div><span><pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.predict(<span class="hljs-string">"Which baking dish is best to bake a banana bread ?"</span>, k=<span class="hljs-number">-1</span>, threshold=<span class="hljs-number">0.5</span>)<br />((<span class="hljs-string">u''</span>__label__baking, <span class="hljs-string">u'__label__bananas'</span>, <span class="hljs-string">u'__label__bread'</span>), array([<span class="hljs-number">1.00000</span>, <span class="hljs-number">0.939923</span>, <span class="hljs-number">0.592677</span>]))<br /></code></pre>
</span></div></div></div></div><div class="tabs"><div class="nav-tabs"><div id="tab-group-61-tab-62" class="nav-link active" data-group="group_61" data-tab="tab-group-61-content-62">Command line</div><div id="tab-group-61-tab-63" class="nav-link" data-group="group_61" data-tab="tab-group-61-content-63">Python</div></div><div class="tab-content"><div id="tab-group-61-content-62" class="tab-pane active" data-group="group_61" tabindex="-1"><div><span><p><br />
We can also evaluate our results with the <code>test</code> command :</p>
<pre><code class="hljs css language-bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid -1 0.5<br />N 3000<br />P@-1  0.702<br />R@-1  0.2<br />Number of examples: 3000<br /></code></pre>
<p>and play with the threshold to obtain desired precision/recall metrics :</p>
<pre><code class="hljs css language-bash">&gt;&gt; ./fasttext <span class="hljs-built_in">test</span> model_cooking.bin cooking.valid -1 0.1<br />N 3000<br />P@-1  0.591<br />R@-1  0.272<br />Number of examples: 3000<br /></code></pre>
</span></div></div><div id="tab-group-61-content-63" class="tab-pane" data-group="group_61" tabindex="-1"><div><span><p><br />
We can also evaluate our results with the <code>test</code> function:</p>
<pre><code class="hljs css language-py"><span class="hljs-meta">&gt;&gt;&gt; </span>model.test(<span class="hljs-string">"cooking.valid"</span>, k=<span class="hljs-number">-1</span>)<br />(<span class="hljs-number">3000L</span>, <span class="hljs-number">0.702</span>, <span class="hljs-number">0.2</span>)<br /></code></pre>
</span></div></div></div></div>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>In this tutorial, we gave a brief overview of how to use fastText to train powerful text classifiers. We had a light overview of some of the most important options to tune.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/en/options.html"><span class="arrow-prev">← </span><span>List of options</span></a><a class="docs-next button" href="/docs/en/unsupervised-tutorial.html"><span>Word representations</span><span class="arrow-next"> →</span></a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/fasttext-icon-white-web.png" alt="fastText"/></a><div><h5>Support</h5><a href="/docs/en/support.html">Getting Started</a><a href="/docs/en/supervised-tutorial.html">Tutorials</a><a href="/docs/en/faqs.html">FAQs</a><a href="/docs/en/api.html">API</a></div><div><h5>Community</h5><a href="https://www.facebook.com/groups/1174547215919768/" target="_blank">Facebook Group</a><a href="http://stackoverflow.com/questions/tagged/fasttext" target="_blank">Stack Overflow</a><a href="https://groups.google.com/forum/#!forum/fasttext-library" target="_blank">Google Group</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/facebookresearch/fastText" target="_blank">GitHub</a><a class="github-button" href="https://github.com/facebookresearch/fastText/" data-icon="octicon-star" data-count-href="/fastText/stargazers" data-count-api="/repos/fastText#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://code.facebook.com/projects/" target="_blank" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2019 Facebook Inc.</section></footer></div></body></html>